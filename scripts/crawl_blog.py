"""ë³´ë³´ìŒ¤ ë„¤ì´ë²„ ë¸”ë¡œê·¸ RSS í¬ë¡¤ë§ â†’ DB ì €ì¥.

Usage:
    python scripts/crawl_blog.py
"""
from __future__ import annotations

import re
import sys
import time
import xml.etree.ElementTree as ET
from html import unescape
from pathlib import Path
from urllib.request import Request, urlopen

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from naverblog.database import Database

BLOG_ID = "byhur99"
RSS_URL = f"https://rss.blog.naver.com/{BLOG_ID}.xml"
# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ API (ë¹„ê³µê°œ APIì§€ë§Œ ì½ê¸° ê°€ëŠ¥)
POST_VIEW_URL = "https://blog.naver.com/PostView.naver?blogId={blog_id}&logNo={post_id}&directAccess=false"


def fetch_url(url: str) -> str:
    """URLì—ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°."""
    req = Request(url, headers={
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    })
    with urlopen(req, timeout=15) as resp:
        return resp.read().decode("utf-8", errors="replace")


def parse_rss(xml_text: str) -> list[dict]:
    """RSS XML íŒŒì‹± â†’ í¬ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸."""
    root = ET.fromstring(xml_text)
    posts = []
    for item in root.findall(".//item"):
        title = item.findtext("title", "").strip()
        link = item.findtext("link", "").strip()
        desc = item.findtext("description", "").strip()
        pub_date = item.findtext("pubDate", "").strip()
        category = item.findtext("category", "").strip()

        # postId ì¶”ì¶œ (URL: /byhur99/224153699867?fromRss=...)
        post_id = ""
        if link:
            m = re.search(r"/(\d{10,})", link)
            if m:
                post_id = m.group(1)

        posts.append({
            "post_id": post_id,
            "title": unescape(title),
            "category": unescape(category),
            "link": link,
            "description": unescape(desc),
            "pub_date": pub_date,
        })
    return posts


def strip_html(html: str) -> str:
    """HTML íƒœê·¸ ì œê±° í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ."""
    # script, style íƒœê·¸ ì œê±°
    text = re.sub(r"<(script|style)[^>]*>.*?</\1>", "", html, flags=re.DOTALL | re.IGNORECASE)
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r"<[^>]+>", "\n", text)
    # HTML ì—”í‹°í‹°
    text = unescape(text)
    # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ \t]+", " ", text)
    return text.strip()


def fetch_post_content(blog_id: str, post_id: str) -> str:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ."""
    url = POST_VIEW_URL.format(blog_id=blog_id, post_id=post_id)
    try:
        html = fetch_url(url)
    except Exception as e:
        print(f"  âš ï¸ HTTP ì‹¤íŒ¨: {e}")
        return ""

    # ë°©ë²• 1: se-main-container ì˜ì—­ ì°¾ê¸°
    start_marker = '<div class="se-main-container">'
    start_idx = html.find(start_marker)
    if start_idx >= 0:
        # se-main-container ì´í›„ì˜ ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ
        region = html[start_idx:start_idx + 200000]  # ìµœëŒ€ 200KB
        # í…ìŠ¤íŠ¸ ë‹¨ë½ ì¶”ì¶œ (se-text-paragraph)
        paragraphs = re.findall(
            r'<p[^>]*class="[^"]*se-text-paragraph[^"]*"[^>]*>(.*?)</p>',
            region, re.DOTALL,
        )
        if paragraphs:
            texts = [strip_html(p) for p in paragraphs]
            return "\n".join(t for t in texts if t.strip())

        # se-text-paragraphì´ ì—†ìœ¼ë©´ ì „ì²´ ì˜ì—­ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # SE_DOC_FOOTER_START ë˜ëŠ” ë‹¤ìŒ í° divê¹Œì§€
        end_marker = "<!-- SE_DOC_FOOTER"
        end_idx = region.find(end_marker)
        if end_idx > 0:
            content = region[:end_idx]
        else:
            content = region[:100000]
        return strip_html(content)

    # ë°©ë²• 2: postViewArea (êµ¬í˜• ì—ë””í„°)
    m = re.search(
        r'<div[^>]*id="postViewArea"[^>]*>(.*?)<!-- // -->',
        html, re.DOTALL,
    )
    if m:
        return strip_html(m.group(1))

    # ë°©ë²• 3: body ì „ì²´ì—ì„œ ì¶”ì¶œ (ìµœí›„ ìˆ˜ë‹¨)
    m = re.search(r"<body[^>]*>(.*)</body>", html, re.DOTALL)
    if m:
        text = strip_html(m.group(1))
        if len(text) > 200:
            return text[:8000]

    return ""


def fetch_post_via_mobile(blog_id: str, post_id: str) -> str:
    """ëª¨ë°”ì¼ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (ëŒ€ì•ˆ)."""
    url = f"https://m.blog.naver.com/{blog_id}/{post_id}"
    try:
        html = fetch_url(url)
    except Exception:
        return ""

    # ëª¨ë°”ì¼ ë³¸ë¬¸ ì˜ì—­
    m = re.search(
        r'<div[^>]*class="[^"]*se-main-container[^"]*"[^>]*>(.*?)<div[^>]*class="[^"]*post_footer',
        html, re.DOTALL,
    )
    if m:
        return strip_html(m.group(1))

    m = re.search(r'<div[^>]*id="viewTypeSelector"[^>]*>(.*?)</div>\s*</div>', html, re.DOTALL)
    if m:
        return strip_html(m.group(1))

    return ""


def main():
    db = Database()
    existing_count = db.count_blog_posts()
    print(f"ğŸ“Š í˜„ì¬ DBì— ì €ì¥ëœ í¬ìŠ¤íŠ¸: {existing_count}ê°œ")

    # 1. RSS ê°€ì ¸ì˜¤ê¸°
    print(f"\nğŸ“¡ RSS í”¼ë“œ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {RSS_URL}")
    rss_xml = fetch_url(RSS_URL)
    posts = parse_rss(rss_xml)
    print(f"âœ… RSSì—ì„œ {len(posts)}ê°œ í¬ìŠ¤íŠ¸ ë°œê²¬")

    # 2. ê° í¬ìŠ¤íŠ¸ ë³¸ë¬¸ í¬ë¡¤ë§
    success = 0
    skip = 0
    fail = 0

    for i, post in enumerate(posts, 1):
        post_id = post["post_id"]
        if not post_id:
            print(f"  [{i}/{len(posts)}] âš ï¸ postId ì—†ìŒ: {post['title']}")
            fail += 1
            continue

        # ì´ë¯¸ DBì— ìˆìœ¼ë©´ ìŠ¤í‚µ
        if db.get_blog_post(post_id):
            print(f"  [{i}/{len(posts)}] â­ï¸ ì´ë¯¸ ì €ì¥ë¨: {post['title'][:30]}")
            skip += 1
            continue

        print(f"  [{i}/{len(posts)}] ğŸ“¥ í¬ë¡¤ë§: {post['title'][:40]}...", end=" ")

        # ë°ìŠ¤í¬í†± ë²„ì „ ë¨¼ì € ì‹œë„
        content = fetch_post_content(BLOG_ID, post_id)

        # ì‹¤íŒ¨í•˜ë©´ ëª¨ë°”ì¼ ë²„ì „ ì‹œë„
        if not content or len(content) < 100:
            content = fetch_post_via_mobile(BLOG_ID, post_id)

        if content and len(content) >= 50:
            db.save_blog_post(
                post_id=post_id,
                title=post["title"],
                category=post["category"],
                content=content,
                pub_date=post["pub_date"],
                link=post["link"],
            )
            print(f"âœ… ({len(content)}ì)")
            success += 1
        else:
            # descriptionì´ë¼ë„ ì €ì¥
            if post["description"] and len(post["description"]) > 20:
                db.save_blog_post(
                    post_id=post_id,
                    title=post["title"],
                    category=post["category"],
                    content=post["description"],
                    pub_date=post["pub_date"],
                    link=post["link"],
                )
                print(f"âš ï¸ descriptionë§Œ ì €ì¥ ({len(post['description'])}ì)")
                success += 1
            else:
                print("âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
                fail += 1

        # rate limit ë°©ì§€
        time.sleep(0.5)

    # 3. ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*50}")
    print(f"ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
    print(f"  âœ… ì„±ê³µ: {success}ê°œ")
    print(f"  â­ï¸ ì´ë¯¸ ì €ì¥ë¨: {skip}ê°œ")
    print(f"  âŒ ì‹¤íŒ¨: {fail}ê°œ")
    print(f"  ğŸ“¦ ì´ DB í¬ìŠ¤íŠ¸: {db.count_blog_posts()}ê°œ")

    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥ëœ ê¸€ ìˆ˜:")
    for cat in db.get_blog_post_categories():
        posts_in_cat = db.list_blog_posts(category=cat)
        print(f"  - {cat}: {len(posts_in_cat)}ê°œ")


if __name__ == "__main__":
    main()
